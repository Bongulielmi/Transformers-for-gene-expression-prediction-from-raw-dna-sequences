<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Documentation - Bioinformatics Project #6
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important;position:relative}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit;overflow:visible}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.figma{display:table;position:relative;width:100%;padding-bottom:56.25%}.figma iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%;border:1px solid #eee}.MJX_Assistive_MathML{display:none}#MathJax_Message{z-index:1000!important}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc.both-mode{margin-left:8px}.ui-toc.both-mode .ui-toc-label{height:40px;padding:10px 4px;border-top-left-radius:0;border-bottom-left-radius:0}.ui-toc-label{background-color:#e6e6e6;border:none;color:#868686;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:20px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:50%;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-more-info{color:#888;cursor:pointer;vertical-align:middle}.ui-more-info .fa{font-size:16px}.ui-connectedGithub,.ui-published-note{color:#888}.ui-connectedGithub{line-height:23px;white-space:nowrap}.ui-connectedGithub a.file-path{color:#888;text-decoration:none;padding-left:22px}.ui-connectedGithub a.file-path:active,.ui-connectedGithub a.file-path:hover{color:#888;text-decoration:underline}.ui-connectedGithub .fa{font-size:20px}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}.selectable{-webkit-user-select:text;-o-user-select:text;user-select:text}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}.site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;letter-spacing:.025em;font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled" data-hard-breaks="true"><p><a href="https://drive.google.com/drive/folders/198skZ1vm1YUYjqVbMe2oC8g4x-i7EEe2?usp=sharing" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Drive" loading="lazy"></a></p><img alt="Keras" src="https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&amp;logo=Keras&amp;logoColor=white"><img alt="TensorFlow" src="https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&amp;logo=TensorFlow&amp;logoColor=white"><img alt="NumPy" src="https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&amp;logo=numpy&amp;logoColor=white"><h2 id="1-Google-Drive-Organization" data-id="1-Google-Drive-Organization"><a class="anchor hidden-xs" href="#1-Google-Drive-Organization" title="1-Google-Drive-Organization"><span class="octicon octicon-link"></span></a><span>1. Google Drive Organization</span></h2><p data-original-title="" title=""><span>Our work took place on the </span><strong><span>Google Drive</span></strong><span> environment, in order to collaborate each other and keep the development and the cooperation fast and easy.</span><br>
<span>In particular, we exploited the </span><strong><span>Google Colab notebooks</span></strong><span>, because  everyone  can  modify  them  once  they’re  shared  on the  drive,  and  thanks  to  </span><code>import-ipynb</code><span> we can import the notebooks in the workflow of other notebooks, increasing the modularity and the reproducibility of our code.</span></p><p data-original-title="" title=""><span>All the notebooks are contained in the </span><code>Colab</code><span> subfolder of our drive.</span><br>
<span>The folder </span><code>/Classes</code><span> contains the model classes and other useful classes like the </span><code>DataManager</code><span>. Those notebooks are exploited by the several  workflow  notebooks  which  are  located in </span><code>/WORKFLOW_GPU</code><span> and </span><code>/WORKFLOW_TPU</code><br>
<span>The most important notebooks are:</span></p><ul data-original-title="" title="">
<li><code>DataManager.ipynb</code><span>: used to manage all the datasets;</span></li>
<li><code>CNN1D.ipynb</code><span>:  contains  a  class  that  manages  all  our  convolutional  solutions  (DivideEtImpera  included),  and  areproduction of the ”Xpresso” original neural network;</span></li>
<li><code>BioLSTM.ipynb</code><span>: contains a class that manages our main LSTM solutions;</span></li>
<li><code>Transformer.ipynb</code><span>: contains a class that handle our transformer ssolutions.</span></li>
</ul><p><span>The  </span><code>Workflow</code><span>  notebooks  are  organized  as  follows:</span></p><ul>
<li><span>at the  beginning  we  have  a  call  to  the  DataManager  in  order to  retrieve  useful  data</span></li>
<li><span>then, we  choose  the  best  suited model  for  our  research  choosing  between  the  ones  available in </span><code>projCNN1D</code><span>, </span><code>projTransformer</code><span>, and </span><code>BioLSTM</code><span> class.</span></li>
</ul><p data-original-title="" title=""><span>The  construction  of  our  workflows  gave  us  the  possibility to  implement  any  kind  of  Deep  Neural  Network  and  change parameters in matters of seconds.</span><br>
<span>The  workflow  name  suggests  the  model  and  the  data  used</span></p><pre><code class="python hljs">{
    <span class="hljs-string">"P"</span> : <span class="hljs-string">"Promoter"</span>, 
    <span class="hljs-string">"H"</span> : <span class="hljs-string">"Half−Life"</span>, 
    <span class="hljs-string">"T"</span> : <span class="hljs-string">"TranscriptionFactors"</span>,
    <span class="hljs-string">"M"</span> : <span class="hljs-string">"MicroRNA"</span>
}
</code></pre><p><span>Example:</span><br>
<code>Workflow_CNN1D_PH_GPU</code><span> uses one or more models defined in CNN1D, it takes Promoters  and  Halflife features, and deploy the models on GPU.</span></p><h2 id="DeepLncLoc-from-scratch" data-id="DeepLncLoc-from-scratch"><a class="anchor hidden-xs" href="#DeepLncLoc-from-scratch" title="DeepLncLoc-from-scratch"><span class="octicon octicon-link"></span></a><span>DeepLncLoc from scratch</span></h2><p data-original-title="" title=""><span>This is a notebook that reimplement from scratch the full pipeline of the DeepLncLoc embedding and can be found in the </span><code>./Colab/varie</code><span> folder.</span><br>
<span>It can be used to retrain the Word2Vec model and to generate dataset with different parameters.</span></p><h2 id="2-Classes" data-id="2-Classes"><a class="anchor hidden-xs" href="#2-Classes" title="2-Classes"><span class="octicon octicon-link"></span></a><span>2. Classes</span></h2><h3 id="21-DataManager-class" data-id="21-DataManager-class"><a class="anchor hidden-xs" href="#21-DataManager-class" title="21-DataManager-class"><span class="octicon octicon-link"></span></a><span>2.1 DataManager class</span></h3><p><span>The </span><strong><span>DataManager</span></strong><span> class manages and imports the dataset in various forms, depending on the model that will be used for training.</span></p><pre data-original-title="" title=""><code class="python hljs">dm = DataManager(
    datadir         = datadir, 
    transformer     = <span class="hljs-literal">False</span>, 
    micro           = <span class="hljs-literal">False</span>, 
    tf              = <span class="hljs-literal">False</span>, 
    datadir_micro   = <span class="hljs-string">"Dataset/microRNA FINALE"</span>, 
    datadir_tf      = <span class="hljs-string">"Dataset/dataset_aumentati"</span>, 
    remove_indicted = <span class="hljs-literal">False</span>,
    DeepLncLoc      = <span class="hljs-literal">False</span>,
)

dataset = dm.get_train(
    np_format  = <span class="hljs-literal">True</span>,  <span class="hljs-comment"># boolean to transform in numpy</span>
    translated = <span class="hljs-literal">False</span>, <span class="hljs-comment"># sequence data translated into categorical (e.g. for transformers)</span>
    micro      = <span class="hljs-literal">False</span>  <span class="hljs-comment"># if True, it returns also microRNA</span>
)

dataset = dm.get_train(
    np_format  = <span class="hljs-literal">True</span>, 
    translated = <span class="hljs-literal">False</span>,
    micro      = <span class="hljs-literal">False</span> 
)

dataset = dm.get_train(
    np_format  = <span class="hljs-literal">True</span>, 
    translated = <span class="hljs-literal">False</span>,
    micro      = <span class="hljs-literal">False</span> 
)
</code></pre><table data-original-title="" title="">
<thead>
<tr>
<th><span>Parameter</span></th>
<th><span>Description</span></th>
<th><span>Values</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>datadir</span></strong></td>
<td><em><span>choose the directory of the dataset</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>transformer</span></strong></td>
<td><em><span>sequence data translated into categorical</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>micro</span></strong></td>
<td><em><span>retrieve also microRNA data</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>tf</span></strong></td>
<td><em><span>retrieve also Transcription Factor data</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>datadir_micro</span></strong></td>
<td><em><span>directory to the microRNA data</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>datadir_tf</span></strong></td>
<td><em><span>directory to the Transcription Factor data</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>remove_indicted</span></strong></td>
<td><em><span>keep only sequences of lenght 20_000</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>DeeplncLoc</span></strong></td>
<td><em><span>retrieve DeepLncLoc embedded sequences (e.g. for our LSTM solution)</span></em></td>
<td><span>boolean</span></td>
</tr>
</tbody>
</table><p><span>The DataManager reads the different </span><code>.h5 format</code><span> datasets from the </span><code>Dataset/</code><span> folder.</span></p><p><span>The standard data are taken from (Xpresso Original Dataset):</span></p><pre><code>- "Dataset/pM10Kb_1KTest/train.h5"
- "Dataset/pM10Kb_1KTest/valid.h5"
- "Dataset/pM10Kb_1KTest/test.h5"
</code></pre><p><span>The data integrated with microRNA are taken from:</span></p><pre><code>- "Dataset/microRNA FINALE/train.h5"
- "Dataset/microRNA FINALE/valid.h5"
- "Dataset/microRNA FINALE/test.h5"
</code></pre><p><span>The data integrated with Transcription Factors are taken from:</span></p><pre><code>- "Dataset/dataset_aumentati/train_tf.h5"
- "Dataset/dataset_aumentati/validation_tf.h5"
- "Dataset/dataset_aumentati/test_tf.h5"
</code></pre><p><span>The standard translated data (for Transformers) are taken from:</span></p><pre><code>- "Dataset/dataset_aumentati/translated_transformers.h5"
</code></pre><p><span>The standard translated data (for Transformers) integrated with Transcription Factors are taken from:</span></p><pre><code>- "Dataset/dataset_aumentati/translated_transformers_tf.h5"
</code></pre><h3 id="22-projCNN1D-class" data-id="22-projCNN1D-class"><a class="anchor hidden-xs" href="#22-projCNN1D-class" title="22-projCNN1D-class"><span class="octicon octicon-link"></span></a><span>2.2 projCNN1D class</span></h3><p><span>The </span><strong><span>projCNN1D</span></strong><span> class exposes the models based on </span><em><span>Convolutional 1D Neural Networks</span></em><span>.</span></p><pre><code class="python hljs">model = projCNN1D(
    checkpoint_dir     = <span class="hljs-string">""</span>,
    model_type         = <span class="hljs-string">"Xpresso"</span>,
    n_epochs           = <span class="hljs-number">10</span>, 
    batch_size         = <span class="hljs-number">32</span>, 
    learning_rate      = <span class="hljs-number">5e-4</span>,
    momentum           = <span class="hljs-number">0.9</span>,
    CNN_input          = (<span class="hljs-number">10_500</span>, <span class="hljs-number">4</span>),
    miRNA_input        = (<span class="hljs-number">2064</span>,),
    lr_reduction_epoch = <span class="hljs-literal">None</span>,
    dropout_rate       = <span class="hljs-number">0.4</span>,
    shuffle            = <span class="hljs-literal">True</span>,
    logdir             = <span class="hljs-literal">None</span>,
    patience           = <span class="hljs-number">30</span>,
    opt                = <span class="hljs-string">"SGD"</span>,
    loss               = <span class="hljs-string">"mse"</span>
)
</code></pre><table data-original-title="" title="">
<thead>
<tr>
<th><span>Parameter</span></th>
<th><span>Description</span></th>
<th><span>Values</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>model_type</span></strong></td>
<td><em><span>choose the correct model for each dataset</span></em></td>
<td><span>‘Xpresso’, ‘Xpresso_nohalf’, ‘Xpresso_TF’, ‘Xpresso_micro’, ‘Xpresso_ort’, ‘DivideEtImpera’, ‘DivideEtImpera_TF’, ‘DivideEtImpera_onlyPromo’</span></td>
</tr>
<tr>
<td><strong><span>n_epochs</span></strong></td>
<td><em><span>number of epochs</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>batch_size</span></strong></td>
<td><em><span>size of the batch</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>CNN_input</span></strong></td>
<td><em><span>shape of the sequence data</span></em></td>
<td><span>(n,m)</span></td>
</tr>
<tr>
<td><strong><span>dropout_rate</span></strong></td>
<td><em><span>select the dropout for the Dense layers</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>logdir</span></strong></td>
<td><em><span>tensorboard directory, if this parameter is set to None, the tensorboard_callback will not be used by the model</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>checkpoint_dir</span></strong></td>
<td><em><span>directory in which is saved the best model</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>patience</span></strong></td>
<td><em><span>patience for the earlystopping</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>learning_rate</span></strong></td>
<td><em><span>learning rate</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>lr_reduction_epoch</span></strong></td>
<td><em><span>milestone where lr_scheduler is invoked to reduce learning rate</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>momentum</span></strong></td>
<td><em><span>momentumtum for the SGD optimizer</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>shuffle</span></strong></td>
<td><em><span>data shuffle</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>opt</span></strong></td>
<td><em><span>choose the optimizer</span></em></td>
<td><span>‘SGD’, anything else(=‘Adam’)</span></td>
</tr>
</tbody>
</table><hr><table>
<thead>
<tr>
<th><span>model_type</span></th>
<th><span>Description</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>Xpresso</span></strong></td>
<td><em><span>official Xpresso model</span></em></td>
</tr>
<tr>
<td><strong><span>Xpresso_nohalf</span></strong></td>
<td><em><span>Xpresso with input only promoters</span></em></td>
</tr>
<tr>
<td><strong><span>Xpresso_TF</span></strong></td>
<td><em><span>Xpresso with Promoter, Half-life and TF as input</span></em></td>
</tr>
<tr>
<td><strong><span>Xpresso_micro</span></strong></td>
<td><em><span>Xpresso with Promoter, Half-life and microRNA as input</span></em></td>
</tr>
<tr>
<td><strong><span>Xpresso_ort</span></strong></td>
<td><em><span>Basically it is equal to Xpresso, but with an orthogonal initialization of the weight</span></em></td>
</tr>
<tr>
<td><strong><span>DivideEtImpera</span></strong></td>
<td><em><span>DivideEtImpera with only Promoters and Half-life data as input</span></em></td>
</tr>
<tr>
<td><strong><span>DivideEtImpera_TF</span></strong></td>
<td><em><span>DivideEtImpera model with also TF data as input</span></em></td>
</tr>
<tr>
<td><strong><span>DivideEtImpera_onlyPromo</span></strong></td>
<td><em><span>DivideEtImpera with only promoters as input</span></em></td>
</tr>
</tbody>
</table><h3 id="23-BioLSTM-class" data-id="23-BioLSTM-class"><a class="anchor hidden-xs" href="#23-BioLSTM-class" title="23-BioLSTM-class"><span class="octicon octicon-link"></span></a><span>2.3 BioLSTM class</span></h3><p><span>The </span><strong><span>BioLSTM</span></strong><span> class exposes the models based on </span><em><span>LSTM block(s)</span></em><span>.</span></p><pre><code class="python hljs">model = BioLSTM(
    model_type = <span class="hljs-string">'classic'</span>, 
    n_epochs   = <span class="hljs-number">50</span>, 
    batch_size = <span class="hljs-number">128</span>, 
    timestep   = <span class="hljs-number">210</span>, 
    features   = <span class="hljs-number">64</span>,
    datadir    = <span class="hljs-string">'Dataset/embedded_data'</span>, 
    opt        = <span class="hljs-string">'adam'</span>, 
    lr         = <span class="hljs-number">3e-4</span>
)
</code></pre><table data-original-title="" title="">
<thead>
<tr>
<th><span>Parameter</span></th>
<th><span>Description</span></th>
<th><span>Values</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>model_type</span></strong></td>
<td><em><span>choose the correct model for each dataset</span></em></td>
<td><span>‘classic’, ‘tf’, ‘only promoter’</span></td>
</tr>
<tr>
<td><strong><span>n_epochs</span></strong></td>
<td><em><span>number of epochs</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>batch_size</span></strong></td>
<td><em><span>size of the batch</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>timestep</span></strong></td>
<td><em><span>length of the timestep of the sequence data</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>features</span></strong></td>
<td><em><span>length of the features for each timestep in the sequence data</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>datadir</span></strong></td>
<td><em><span>directory in which is saved the best model</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>opt</span></strong></td>
<td><em><span>optimizer used by the model</span></em></td>
<td><span>‘adam’, ‘sgd’</span></td>
</tr>
<tr>
<td><strong><span>lr</span></strong></td>
<td><em><span>learning rate</span></em></td>
<td><span>float</span></td>
</tr>
</tbody>
</table><hr><table>
<thead>
<tr>
<th><span>model_type</span></th>
<th><span>Description</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>classic</span></strong></td>
<td><em><span>BioLSTM model with input promoters and Half-life data</span></em></td>
</tr>
<tr>
<td><strong><span>tf</span></strong></td>
<td><em><span>BioLSTM with Promoter, Half-life data and TF as input</span></em></td>
</tr>
<tr>
<td><strong><span>only promoter</span></strong></td>
<td><em><span>BioLSTM with promoters as input</span></em></td>
</tr>
</tbody>
</table><h3 id="24-projTFNet-class" data-id="24-projTFNet-class"><a class="anchor hidden-xs" href="#24-projTFNet-class" title="24-projTFNet-class"><span class="octicon octicon-link"></span></a><span>2.4 projTFNet class</span></h3><p><span>The </span><strong><span>projTFNet</span></strong><span> class exposes the </span><em><span>Fully Connected Neural Network</span></em><span> to make the regression only on transcription factor data.</span></p><pre><code class="python hljs">model = projTFNet(
    checkpoint_dir     = <span class="hljs-string">""</span>,
    model_type         = <span class="hljs-string">"TF"</span>,
    n_epochs           = <span class="hljs-number">10</span>, 
    batch_size         = <span class="hljs-number">32</span>, 
    learning_rate      = <span class="hljs-number">5e-4</span>,
    momentum           = <span class="hljs-number">0.9</span>,
    lr_reduction_epoch = <span class="hljs-literal">None</span>,
    shuffle            = <span class="hljs-literal">True</span>,
    logdir             = <span class="hljs-literal">None</span>,
    patience           = <span class="hljs-number">30</span>
)
</code></pre><table>
<thead>
<tr>
<th><span>Parameter</span></th>
<th><span>Description</span></th>
<th><span>Values</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>model_type</span></strong></td>
<td><em><span>choose the correct model for each dataset</span></em></td>
<td><span>‘TF’</span></td>
</tr>
<tr>
<td><strong><span>n_epochs</span></strong></td>
<td><em><span>number of epochs</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>batch_size</span></strong></td>
<td><em><span>size of the batch</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>CNN_input</span></strong></td>
<td><em><span>shape of the sequence data</span></em></td>
<td><span>(n,m)</span></td>
</tr>
<tr>
<td><strong><span>dropout_rate</span></strong></td>
<td><em><span>select the dropout for the Dense layers</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>logdir</span></strong></td>
<td><em><span>tensorboard directory</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>checkpoint_dir</span></strong></td>
<td><em><span>directory in which is saved the best model</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>patience</span></strong></td>
<td><em><span>patience for the earlystopping</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>learning_rate</span></strong></td>
<td><em><span>learning rate</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>lr_reduction_epoch</span></strong></td>
<td><em><span>milestone where lr_scheduler is invoked to reduce learning rate</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>momentum</span></strong></td>
<td><em><span>momentumtum for the SGD optimizer</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>shuffle</span></strong></td>
<td><em><span>data shuffle</span></em></td>
<td><span>boolean</span></td>
</tr>
</tbody>
</table><h3 id="25-projTransformer-class" data-id="25-projTransformer-class"><a class="anchor hidden-xs" href="#25-projTransformer-class" title="25-projTransformer-class"><span class="octicon octicon-link"></span></a><span>2.5 projTransformer class</span></h3><p><span>The </span><strong><span>projTransformer</span></strong><span> (</span><code>Classes/Transformer.ipynb</code><span>) class exposes the </span><em><span>Transformers-like models</span></em><span>.</span></p><pre><code class="python hljs">model = projTransformer(
    checkpoint_dir     = <span class="hljs-string">""</span>,
    model_type         = <span class="hljs-string">"best"</span>,
    n_epochs           = <span class="hljs-number">300</span>, 
    batch_size         = <span class="hljs-number">32</span>, 
    learning_rate      = <span class="hljs-number">1e-4</span>,
    momentum           = <span class="hljs-number">0.9</span>,
    maxlen             = <span class="hljs-number">10500</span>,
    embed_dim          = <span class="hljs-number">32</span>,
    num_heads          = <span class="hljs-number">4</span>,
    ff_dim             = <span class="hljs-number">64</span>,
    vocab_size         = <span class="hljs-number">5</span>,
    dense              = <span class="hljs-number">64</span>,
    lr_reduction_epoch = <span class="hljs-literal">None</span>,
    dropout_rate       = <span class="hljs-number">0.1</span>,
    t_rate             = <span class="hljs-number">0.1</span>,
    patience           = <span class="hljs-number">20</span>,
    optimizer          = <span class="hljs-string">"SGD"</span>,
    warmup_steps       = <span class="hljs-number">8000</span>,
    shuffle            = <span class="hljs-literal">True</span>,
    loss               = <span class="hljs-string">"mse"</span>,
    logdir             = <span class="hljs-literal">None</span>
)
</code></pre><table data-original-title="" title="">
<thead>
<tr>
<th><span>Parameter</span></th>
<th><span>Description</span></th>
<th><span>Values</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>model_type</span></strong></td>
<td><em><span>choose the correct model for each dataset</span></em></td>
<td><span>‘best’, ‘globalP’, ‘posSkip’, ‘DeepLncLoc’, ‘TF’, ‘DeepLncLoc_TF’, ‘onlyPromo’, ‘DeepLncLoc_Promoter’</span></td>
</tr>
<tr>
<td><strong><span>n_epochs</span></strong></td>
<td><em><span>number of epochs</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>batch_size</span></strong></td>
<td><em><span>size of the batch</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>maxlen</span></strong></td>
<td><em><span>length of the sequence data</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>dropout_rate</span></strong></td>
<td><em><span>select the dropout for the Dense layers</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>logdir</span></strong></td>
<td><em><span>tensorboard directory</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>checkpoint_dir</span></strong></td>
<td><em><span>directory in which is saved the best model</span></em></td>
<td><span>str</span></td>
</tr>
<tr>
<td><strong><span>patience</span></strong></td>
<td><em><span>patience for the earlystopping</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>learning_rate</span></strong></td>
<td><em><span>learning rate</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>lr_reduction_epoch</span></strong></td>
<td><em><span>milestone where lr_scheduler is invoked to reduce learning rate</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>momentum</span></strong></td>
<td><em><span>momentum for the SGD optimizer</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>shuffle</span></strong></td>
<td><em><span>data shuffle during train</span></em></td>
<td><span>boolean</span></td>
</tr>
<tr>
<td><strong><span>embed_dim</span></strong></td>
<td><em><span>dimension of the embedding (depth of w2v)</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>num_heads</span></strong></td>
<td><em><span>number of heads</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>ff_dim</span></strong></td>
<td><em><span>number of neurons of the feed forward network</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>vocab_size</span></strong></td>
<td><em><span>used in word2vec</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>dense</span></strong></td>
<td><em><span>number of neurons of Dense layers</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>t_rate</span></strong></td>
<td><em><span>dropout rate used in the dense layers of transformers blocks</span></em></td>
<td><span>float</span></td>
</tr>
<tr>
<td><strong><span>warmup_steps</span></strong></td>
<td><em><span>steps to warmup the learning rate in original transformer scheduler/optimizer</span></em></td>
<td><span>int</span></td>
</tr>
<tr>
<td><strong><span>loss</span></strong></td>
<td><em><span>loss used</span></em></td>
<td><span>instance of tf.keras losses (or just string name e.g. “mse”)</span></td>
</tr>
<tr>
<td><strong><span>optimizer</span></strong></td>
<td><em><span>optimizer used</span></em></td>
<td><span>‘SGD’, ‘Adam’, ‘Adadelta’, ‘Adamax’, ‘Original’</span></td>
</tr>
</tbody>
</table><hr><table>
<thead>
<tr>
<th><span>model_type</span></th>
<th><span>Description</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><span>best</span></strong></td>
<td><em><span>our transformer</span></em></td>
</tr>
<tr>
<td><strong><span>globalP</span></strong></td>
<td><em><span>our transformer, with a global pooling after the transformer block</span></em></td>
</tr>
<tr>
<td><strong><span>posSkip</span></strong></td>
<td><em><span>like model_type “best”, but it skips only the positional embedding</span></em></td>
</tr>
<tr>
<td><strong><span>TF</span></strong></td>
<td><em><span>our transformer, but it integrate also Transcription Factors data</span></em></td>
</tr>
<tr>
<td><strong><span>onlyPromo</span></strong></td>
<td><em><span>our transformer, but only promoters as input</span></em></td>
</tr>
<tr>
<td><strong><span>DeepLncLoc</span></strong></td>
<td><em><span>DeepLncLoc Transformer</span></em></td>
</tr>
<tr>
<td><strong><span>DeepLncLoc_TF</span></strong></td>
<td><em><span>DeepLncLoc Transformer and as input promoters, Half-Life and TF data</span></em></td>
</tr>
<tr>
<td><strong><span>DeepLncLoc_onlyPromo</span></strong></td>
<td><em><span>DeepLncLoc Transformer, but only promoters as input</span></em></td>
</tr>
</tbody>
</table><h2 id="3-Workflow-Example-GPU" data-id="3-Workflow-Example-GPU"><a class="anchor hidden-xs" href="#3-Workflow-Example-GPU" title="3-Workflow-Example-GPU"><span class="octicon octicon-link"></span></a><span>3. Workflow Example (GPU)</span></h2><h3 id="31-Prerequisites" data-id="31-Prerequisites"><a class="anchor hidden-xs" href="#31-Prerequisites" title="31-Prerequisites"><span class="octicon octicon-link"></span></a><span>3.1 Prerequisites</span></h3><ol data-original-title="" title="">
<li><span>Connect your Google Drive account and allow to import all the notebooks (e.g. DataManager, CNN1D)</span></li>
<li><span>Once you have been added to the drive repository, in order to let the import to work properly, you need to add a shortcut of the repository in the main directory of your drive, otherwise the </span><code>%cd</code><span> command will fail and you will not be able to run our notebook properly.</span></li>
</ol><pre><code class="python hljs">!pip install <span class="hljs-keyword">import</span>-ipynb
<span class="hljs-keyword">import</span> import_ipynb

<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> google.colab <span class="hljs-keyword">import</span> drive
drive.mount(<span class="hljs-string">'/content/drive'</span>, force_remount=<span class="hljs-literal">True</span>)

%cd <span class="hljs-string">"drive/MyDrive/Bionformatics_Project/Colab"</span>
</code></pre><h3 id="32-Classes-import" data-id="32-Classes-import"><a class="anchor hidden-xs" href="#32-Classes-import" title="32-Classes-import"><span class="octicon octicon-link"></span></a><span>3.2 Classes import:</span></h3><pre><code class="python hljs"><span class="hljs-keyword">from</span> Classes.DataManager <span class="hljs-keyword">import</span> DataManager
<span class="hljs-keyword">from</span> Classes.Transformer <span class="hljs-keyword">import</span> projTransformer
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

%load_ext tensorboard
</code></pre><h3 id="33-Retrieve-training-data-from-the-Data-Manager" data-id="33-Retrieve-training-data-from-the-Data-Manager"><a class="anchor hidden-xs" href="#33-Retrieve-training-data-from-the-Data-Manager" title="33-Retrieve-training-data-from-the-Data-Manager"><span class="octicon octicon-link"></span></a><span>3.3 Retrieve training data from the Data Manager:</span></h3><pre><code class="python hljs">dm = DataManager(transformer=<span class="hljs-literal">False</span>, micro=<span class="hljs-literal">False</span>)

X_trainhalflife, X_trainpromoter, y_train, _, _                 = dm.get_train(<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>)
X_validationhalflife, X_validationpromoter, y_validation, _, _  = dm.get_validation(<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>)
X_testhalflife, X_testpromoter, y_test, _, _                    = dm.get_test(<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>)
</code></pre><pre><code class="python hljs">leftpos  = <span class="hljs-number">3_500</span>          
rightpos = <span class="hljs-number">13_500</span>         
maxlen   = rightpos-leftpos
</code></pre><pre><code class="python hljs">X_trainpromoter      = X_trainpromoter[:, leftpos:rightpos, :]
X_validationpromoter = X_validationpromoter[:, leftpos:rightpos, :]
X_testpromoter       = X_testpromoter[:, leftpos:rightpos, :]
</code></pre><h3 id="34-Train-the-model" data-id="34-Train-the-model"><a class="anchor hidden-xs" href="#34-Train-the-model" title="34-Train-the-model"><span class="octicon octicon-link"></span></a><span>3.4 Train the model:</span></h3><pre><code class="python hljs">model_type     = <span class="hljs-string">"Xpresso"</span>
checkpoint_dir = <span class="hljs-string">"myFirstTrain/"</span>
logdir         = <span class="hljs-string">"tensorboardDir/"</span>

net = projCNN1D(
    checkpoint_dir =checkpoint_dir, 
    model_type     = model_type, 
    n_epochs       = <span class="hljs-number">300</span>, 
    batch_size     = <span class="hljs-number">256</span>, 
    learning_rate  = <span class="hljs-number">5e-4</span>, 
    CNN_input      = (maxlen, <span class="hljs-number">4</span>), 
    dropout_rate   = <span class="hljs-number">0.5</span>, 
    logdir         = logdir
)
        
net.train_model(
    [X_trainpromoter, X_trainhalflife],
    y_train, 
    [X_validationpromoter, X_validationhalflife], 
    y_validation
)
</code></pre><h3 id="35-Evaluate-the-model" data-id="35-Evaluate-the-model" data-original-title="" title=""><a class="anchor hidden-xs" href="#35-Evaluate-the-model" title="35-Evaluate-the-model"><span class="octicon octicon-link"></span></a><span>3.5 Evaluate the model:</span></h3><h4 id="351-At-the-last-epoch" data-id="351-At-the-last-epoch"><a class="anchor hidden-xs" href="#351-At-the-last-epoch" title="351-At-the-last-epoch"><span class="octicon octicon-link"></span></a><span>3.5.1 At the last epoch:</span></h4><pre data-original-title="" title=""><code class="python hljs">net.evaluate([X_testpromoter, X_testhalflife], y_test)
</code></pre><h4 id="352-At-the-epoch-with-lowest-validation-loss" data-id="352-At-the-epoch-with-lowest-validation-loss"><a class="anchor hidden-xs" href="#352-At-the-epoch-with-lowest-validation-loss" title="352-At-the-epoch-with-lowest-validation-loss"><span class="octicon octicon-link"></span></a><span>3.5.2 At the epoch with lowest validation loss:</span></h4><pre><code class="python hljs">net.evaluate_best([X_testpromoter, X_testhalflife], y_test)
</code></pre><h3 id="36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target" data-id="36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target"><a class="anchor hidden-xs" href="#36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target" title="36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target"><span class="octicon octicon-link"></span></a><span>3.6 Visualize the distribution of the predictions with respect to the target:</span></h3><pre><code class="python hljs">net.plot_train()
net.plot_r2([X_testpromoter, X_testhalflife], y_test)
net.plot_kde([X_testpromoter, X_testhalflife], y_test)
</code></pre><ul>
<li><span>plot_r2 shows a scatter plot of the distance of the prediction from the true values</span></li>
<li><span>plt_kde match the kernel density distribution of the predicted values and the targets</span></li>
</ul><h2 id="4-Workflow-Example-TPU" data-id="4-Workflow-Example-TPU"><a class="anchor hidden-xs" href="#4-Workflow-Example-TPU" title="4-Workflow-Example-TPU"><span class="octicon octicon-link"></span></a><span>4. Workflow Example (TPU)</span></h2><h3 id="41-Prerequisites" data-id="41-Prerequisites"><a class="anchor hidden-xs" href="#41-Prerequisites" title="41-Prerequisites"><span class="octicon octicon-link"></span></a><span>4.1 Prerequisites:</span></h3><p data-original-title="" title=""><span>The TPU workflow configuration is barely the same of the GPU one, you have to just to be aware of defining the model into the TPU scope, like here:</span></p><pre><code>import tensorflow as tf
tf.keras.backend.clear_session()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

strategy = tf.distribute.experimental.TPUStrategy(resolver)


with strategy.scope():
    net = projTransformer(checkpoint_dir="batch128/",model_type=model_type, n_epochs=500, 
                      batch_size=256, learning_rate=5e-4, lr_reduction_epoch=200, 
                      maxlen=maxlen, embed_dim=64, num_heads=4, ff_dim=384, dense=100, 
                      dropout_rate=0.1, optimizer="Adam", warmup_steps=4_000, patience=20)
</code></pre><h3 id="42-Considerations" data-id="42-Considerations"><a class="anchor hidden-xs" href="#42-Considerations" title="42-Considerations"><span class="octicon octicon-link"></span></a><span>4.2 Considerations:</span></h3><ul data-original-title="" title="">
<li><span>You do not need to call </span><code>net.train_model()</code><span> into the TPU scope, and we suggest to call it outside, maybe in the following notebook cell;</span></li>
<li><span>You may need to increase the </span><code>batch_size</code><span> in order to exploit a higher parallelization. A rule of thumb suggests to multuply by an 8 factor the </span><code>batch_size</code><span>. In this project we used the same </span><code>batch_size</code><span> that we used with GPU models in order to avoid a further hyperparameter research, giving the fact that the TPU models gives the same results when deployed on GPU if they share the same hyperparameters. Obviously, even if the </span><code>batch_size</code><span> is the same, the model will train faster on TPU.</span></li>
<li><span>Due to the fact that the model runs on the TPU cloud, we cannot exploit some callbacks like </span><code>ModelCheckpoint</code><span>. So we decided to use a parameter of </span><code>EarlyStopping</code><span> which allow us to restore the net’s weight of of the epoch with mininum validation loss. As a consequence, once the training is completed, </span><code>net.evaluate()</code><span> and </span><code>net.evaluate_best()</code><span> will return the same value. Moreover, net.model.save() will not work for the same reasons.</span></li>
<li><code>Tensorboard</code><span> cannot be exploited when you run on TPU, for the same reason we cited before (TPU models runs on cloud)</span></li>
<li><code>BioLSTM</code><span> has no the TPU counterpart because it is alredy fast when deployed on GPU, so it would have been useless to implement it.</span></li>
<li><code>remove_indicted</code><span> option work only on for the Xpresso dataset. If you want to train our transformer with the transcription factor dataset you will need to set </span><code>vocab_size = 5</code><span>.</span></li>
</ul><h2 id="5-Further-Considerations" data-id="5-Further-Considerations"><a class="anchor hidden-xs" href="#5-Further-Considerations" title="5-Further-Considerations"><span class="octicon octicon-link"></span></a><span>5. Further Considerations:</span></h2><p data-original-title="" title=""><span>When we started this project, the latest tensorflow version was 2.4.2, but after two months it changed to 2.5.x, so if you have problem in loading our </span><code>Saved_Models</code><span>, you may need to force the installation of the correct tensorflow library.</span></p><pre><code>!pip install tensorflow==2.4.2

</code></pre><h2 id="6-Best-HyperParameters" data-id="6-Best-HyperParameters"><a class="anchor hidden-xs" href="#6-Best-HyperParameters" title="6-Best-HyperParameters"><span class="octicon octicon-link"></span></a><span>6. Best HyperParameters:</span></h2><pre><code>BioLSTM (BioLSTM in BioLSTM.ipynb):
    model_type = "classic"
    n_epochs = 140
    batch_size = 128
    learning_rate = 3e-4
    patience = 15
    optimizer = "Adam"
    timestep = 210
    features = 64

Our_Transformer (projTransformer in Transformer.ipynb):
    model_type = "best"
    n_epochs = 300
    batch_size = 256
    learning_rate = 1e-3
    patience = 20
    optimizer = "SGD"
    lr_reduction_epoch = 60
    embed_dim = 32
    num_heads = 1
    vocab_size = 4 (#increase this value if you feed the network with more than 4 symbols)
    ff_dim = 64
    dense = 64
    dropout_rate = 0.1
    t_rate = 0.1
    momentum = 0.9

DivideEtImpera (projCNN1D in CNN1D.ipynb):
    model_type = "DivideEtImpera"
    n_epochs = 300
    batch_size = 256
    learning_rate = 1e-3
    patience = 15
    optimizer = "SGD"
    lr_reduction_epoch = 70
    momentum = 0.9

Transformer DeepLncLoc (projTransformer in Transformer.ipynb):
    model_type = "DeepLncLoc"
    n_epochs = 300
    batch_size = 256
    learning_rate = 5e-4
    patience = 20
    optimizer = "Adam"
    lr_reduction_epoch = None
    embed_dim = 64
    num_heads = 4
    ff_dim = 384
    dense = 100
    dropout_rate = 0.1
</code></pre><h3 id="NB" data-id="NB"><a class="anchor hidden-xs" href="#NB" title="NB"><span class="octicon octicon-link"></span></a><span>N.B:</span></h3><h3 id="The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class" data-id="The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class" data-original-title="" title=""><a class="anchor hidden-xs" href="#The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class" title="The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class"><span class="octicon octicon-link"></span></a><span>The parameters that are not present in these lists have to be assumed as the predefined parameters of the respective class.</span></h3></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#1-Google-Drive-Organization" title="1. Google Drive Organization">1. Google Drive Organization</a></li>
<li class=""><a href="#DeepLncLoc-from-scratch" title="DeepLncLoc from scratch">DeepLncLoc from scratch</a></li>
<li class=""><a href="#2-Classes" title="2. Classes">2. Classes</a><ul class="nav">
<li class=""><a href="#21-DataManager-class" title="2.1 DataManager class">2.1 DataManager class</a></li>
<li><a href="#22-projCNN1D-class" title="2.2 projCNN1D class">2.2 projCNN1D class</a></li>
<li><a href="#23-BioLSTM-class" title="2.3 BioLSTM class">2.3 BioLSTM class</a></li>
<li><a href="#24-projTFNet-class" title="2.4 projTFNet class">2.4 projTFNet class</a></li>
<li><a href="#25-projTransformer-class" title="2.5 projTransformer class">2.5 projTransformer class</a></li>
</ul>
</li>
<li><a href="#3-Workflow-Example-GPU" title="3. Workflow Example (GPU)">3. Workflow Example (GPU)</a><ul class="nav">
<li><a href="#31-Prerequisites" title="3.1 Prerequisites">3.1 Prerequisites</a></li>
<li><a href="#32-Classes-import" title="3.2 Classes import:">3.2 Classes import:</a></li>
<li><a href="#33-Retrieve-training-data-from-the-Data-Manager" title="3.3 Retrieve training data from the Data Manager:">3.3 Retrieve training data from the Data Manager:</a></li>
<li><a href="#34-Train-the-model" title="3.4 Train the model:">3.4 Train the model:</a></li>
<li><a href="#35-Evaluate-the-model" title="3.5 Evaluate the model:">3.5 Evaluate the model:</a><ul class="nav">
<li><a href="#351-At-the-last-epoch" title="3.5.1 At the last epoch:">3.5.1 At the last epoch:</a></li>
<li><a href="#352-At-the-epoch-with-lowest-validation-loss" title="3.5.2 At the epoch with lowest validation loss:">3.5.2 At the epoch with lowest validation loss:</a></li>
</ul>
</li>
<li><a href="#36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target" title="3.6 Visualize the distribution of the predictions with respect to the target:">3.6 Visualize the distribution of the predictions with respect to the target:</a></li>
</ul>
</li>
<li class=""><a href="#4-Workflow-Example-TPU" title="4. Workflow Example (TPU)">4. Workflow Example (TPU)</a><ul class="nav">
<li class=""><a href="#41-Prerequisites" title="4.1 Prerequisites:">4.1 Prerequisites:</a></li>
<li><a href="#42-Considerations" title="4.2 Considerations:">4.2 Considerations:</a></li>
</ul>
</li>
<li><a href="#5-Further-Considerations" title="5. Further Considerations:">5. Further Considerations:</a></li>
<li class=""><a href="#6-Best-HyperParameters" title="6. Best HyperParameters:">6. Best HyperParameters:</a><ul class="nav">
<li><a href="#NB" title="N.B:">N.B:</a></li>
<li class=""><a href="#The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class" title="The parameters that are not present in these lists have to be assumed as the predefined parameters of the respective class.">The parameters that are not present in these lists have to be assumed as the predefined parameters of the respective class.</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li class=""><a href="#1-Google-Drive-Organization" title="1. Google Drive Organization">1. Google Drive Organization</a></li>
<li class=""><a href="#DeepLncLoc-from-scratch" title="DeepLncLoc from scratch">DeepLncLoc from scratch</a></li>
<li class=""><a href="#2-Classes" title="2. Classes">2. Classes</a><ul class="nav">
<li class=""><a href="#21-DataManager-class" title="2.1 DataManager class">2.1 DataManager class</a></li>
<li><a href="#22-projCNN1D-class" title="2.2 projCNN1D class">2.2 projCNN1D class</a></li>
<li><a href="#23-BioLSTM-class" title="2.3 BioLSTM class">2.3 BioLSTM class</a></li>
<li><a href="#24-projTFNet-class" title="2.4 projTFNet class">2.4 projTFNet class</a></li>
<li><a href="#25-projTransformer-class" title="2.5 projTransformer class">2.5 projTransformer class</a></li>
</ul>
</li>
<li><a href="#3-Workflow-Example-GPU" title="3. Workflow Example (GPU)">3. Workflow Example (GPU)</a><ul class="nav">
<li><a href="#31-Prerequisites" title="3.1 Prerequisites">3.1 Prerequisites</a></li>
<li><a href="#32-Classes-import" title="3.2 Classes import:">3.2 Classes import:</a></li>
<li><a href="#33-Retrieve-training-data-from-the-Data-Manager" title="3.3 Retrieve training data from the Data Manager:">3.3 Retrieve training data from the Data Manager:</a></li>
<li><a href="#34-Train-the-model" title="3.4 Train the model:">3.4 Train the model:</a></li>
<li><a href="#35-Evaluate-the-model" title="3.5 Evaluate the model:">3.5 Evaluate the model:</a><ul class="nav">
<li><a href="#351-At-the-last-epoch" title="3.5.1 At the last epoch:">3.5.1 At the last epoch:</a></li>
<li><a href="#352-At-the-epoch-with-lowest-validation-loss" title="3.5.2 At the epoch with lowest validation loss:">3.5.2 At the epoch with lowest validation loss:</a></li>
</ul>
</li>
<li><a href="#36-Visualize-the-distribution-of-the-predictions-with-respect-to-the-target" title="3.6 Visualize the distribution of the predictions with respect to the target:">3.6 Visualize the distribution of the predictions with respect to the target:</a></li>
</ul>
</li>
<li class=""><a href="#4-Workflow-Example-TPU" title="4. Workflow Example (TPU)">4. Workflow Example (TPU)</a><ul class="nav">
<li class=""><a href="#41-Prerequisites" title="4.1 Prerequisites:">4.1 Prerequisites:</a></li>
<li><a href="#42-Considerations" title="4.2 Considerations:">4.2 Considerations:</a></li>
</ul>
</li>
<li><a href="#5-Further-Considerations" title="5. Further Considerations:">5. Further Considerations:</a></li>
<li class=""><a href="#6-Best-HyperParameters" title="6. Best HyperParameters:">6. Best HyperParameters:</a><ul class="nav">
<li><a href="#NB" title="N.B:">N.B:</a></li>
<li class=""><a href="#The-parameters-that-are-not-present-in-these-lists-have-to-be-assumed-as-the-predefined-parameters-of-the-respective-class" title="The parameters that are not present in these lists have to be assumed as the predefined parameters of the respective class.">The parameters that are not present in these lists have to be assumed as the predefined parameters of the respective class.</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
