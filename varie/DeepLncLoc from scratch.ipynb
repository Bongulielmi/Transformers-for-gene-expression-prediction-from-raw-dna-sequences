{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepLncLoc from scratch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"00emfKjA5dhV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625219447892,"user_tz":-120,"elapsed":7350,"user":{"displayName":"Alessandro Palladini","photoUrl":"","userId":"12617552911803956187"}},"outputId":"3d2df0ee-0e2b-4e4f-c2e3-5de5a6085d26"},"source":["!pip install import-ipynb\n","\n","import import_ipynb\n","from gensim.models import Word2Vec\n","import numpy as np\n","import gc\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import os,logging,pickle,random,torch\n","from matplotlib import pyplot\n","import pandas as pd\n","from scipy import stats\n","import keras\n","import h5py\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Dense, Activation, LSTM, GRU, SimpleRNN, Conv1D, TimeDistributed, MaxPooling1D, Flatten, Dropout, Input, AveragePooling1D, Add, Concatenate\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from google.colab import drive\n","\n","datadir = 'Dataset/embedded_data'\n","drive.mount('/content/drive', force_remount=True)\n","\n","%cd \"drive/MyDrive/Bionformatics_Project/Colab\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1he9S1Es-XalZ9vgCRMbTTNd6cXeRkkFI/Bionformatics_Project/Colab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9aN6unVM5hdH"},"source":["#Calculate embedded dataset for sequences with halflife"]},{"cell_type":"markdown","metadata":{"id":"CVEK5J_F6t69"},"source":["Load the sequences"]},{"cell_type":"code","metadata":{"id":"sXJJItaDXTxE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5fdab99-9fc0-4583-872f-0c6fdc3cbccb"},"source":["from Classes.DataManager import DataManager\n","print(\"> Loading DataManager...\")\n","dm = DataManager(transformer=True, micro = False, tf = False)\n","\n","print(\"> Loading train data...\")\n","X_trainhalflife, X_trainpromoter, y_train, geneName_train, _ = dm.get_train(True, True)\n","\n","print(\"> Loading validation data...\")\n","X_validationhalflife, X_validationpromoter, y_validation, geneName_valid, _ = dm.get_validation(True, True)\n","\n","print(\"> Loading test data...\")\n","X_testhalflife, X_testpromoter, y_test, geneName_test, _ = dm.get_test(True, True)\n","\n","X_trainpromoter = np.array(X_trainpromoter)\n","\n","mask_tr = np.all(X_trainpromoter < 4, axis = 1)\n","X_trainpromoter = X_trainpromoter[mask_tr]\n","X_trainhalflife = X_trainhalflife[mask_tr]\n","geneName_train = geneName_train[mask_tr]\n","y_train = y_train[mask_tr]\n","\n","X_testpromoter = np.array(X_testpromoter)\n","\n","mask_te = np.all(X_testpromoter < 4, axis = 1)\n","X_testpromoter = X_testpromoter[mask_te]\n","X_testhalflife = X_testhalflife[mask_te]\n","geneName_test = geneName_test[mask_te]\n","y_test = y_test[mask_te]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["importing Jupyter notebook from /content/drive/.shortcut-targets-by-id/1he9S1Es-XalZ9vgCRMbTTNd6cXeRkkFI/Bionformatics_Project/Colab/Classes/DataManager.ipynb\n","> Loading DataManager...\n","> Loading train data...\n","> Loading validation data...\n","> Loading test data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TRIioOiC6-Cb"},"source":["Algorithm basic Functions"]},{"cell_type":"code","metadata":{"id":"MqHUH_ThPOfe"},"source":["def chunks(l, n=50):\n","    n = max(1, n)\n","    y = [l[i:i+n] for i in range(0, len(l), n)]\n","    return y\n","\n","def segmentation_train(seq):\n","  segments = [seq[j:j+3] for j in range(len(seq)-2)]\n","  vec = [''.join([str(j) for j in i]) for i in segments]                                       \n","  return vec\n","\n","def segmentation(seq):\n","  segments = [seq[j:j+3] for j in range(len(seq)-2)]\n","  vec = [''.join([str(j) for j in i]) for i in segments]\n","  vec = [w2v[i] for i in vec]  \n","  y = np.array(vec).mean(axis=0)      #capire se va bene o ci vuole il layer apposito\n","  return y\n","\n","def full_map(x):\n","  temp = chunks(x[3000:13500])\n","  temp = map(segmentation,temp)\n","  temp = tuple(temp)\n","  temp = np.stack(temp)\n","  return temp\n","\n","def embeddings(x): \n","  data = np.array([el for el in tqdm(map(full_map, x))])\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5J9RlC-U7Ibx"},"source":["Create the Word2Vec model"]},{"cell_type":"code","metadata":{"id":"docf2crn6dtJ"},"source":["w2v = Word2Vec(sentences=map(segmentation_train,X_trainpromoter), size=64, window=5, min_count=0, workers=4, sg = 1, iter = 10)\n","w2v.save(\"Dataset/embedded_data/word2vec.model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAQAID1Z7O2G"},"source":["Calculate the new embedding"]},{"cell_type":"code","metadata":{"id":"9hO8fSHPqAE5"},"source":["w2v = Word2Vec.load(\"Dataset/embedded_data/word2vec.model\")\n","\n","X_train = embeddings(X_trainpromoter)\n","X_validation = embeddings(X_validationpromoter)\n","X_test = embeddings(X_testpromoter)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5p4aEzGe7VQh"},"source":["Save the new data"]},{"cell_type":"code","metadata":{"id":"NBIlLwfe6hYU"},"source":["h5f = h5py.File('Dataset/embedded_data/etrain.h5', 'w')\n","h5f.create_dataset('promoter', data=X_train)\n","h5f.create_dataset('halflife', data=X_trainhalflife)\n","h5f.create_dataset('label',    data=y_train)\n","h5f.close()\n","\n","h5f = h5py.File('Dataset/embedded_data/etest.h5', 'w')\n","h5f.create_dataset('promoter', data=X_test)\n","h5f.create_dataset('halflife', data=X_testhalflife)\n","h5f.create_dataset('label',    data=y_test)\n","h5f.close()\n","\n","h5f = h5py.File('Dataset/embedded_data/evalidation.h5', 'w')\n","h5f.create_dataset('promoter', data=X_validation)\n","h5f.create_dataset('halflife', data=X_validationhalflife)\n","h5f.create_dataset('label',    data=y_validation)\n","h5f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PdLhjM_G5waW"},"source":["#Calculate embedded dataset for sequences with halflife and transcription factors"]},{"cell_type":"markdown","metadata":{"id":"B8Po_y2u8UNJ"},"source":["Load the data and allign it"]},{"cell_type":"code","metadata":{"id":"FD_X5J3155aG"},"source":["from DataManager import DataManager\n","print(\"> Loading DataManager...\")\n","dm = DataManager(transformer=True)\n","\n","print(\"> Loading train data...\")\n","X_trainhalflife, X_trainpromoter, y_train, geneName_train,_ = dm.get_train(True, True)\n","\n","print(\"> Loading validation data...\")\n","X_validationhalflife, X_validationpromoter, y_validation, geneName_valid,_ = dm.get_validation(True, True)\n","\n","print(\"> Loading test data...\")\n","X_testhalflife, X_testpromoter, y_test, geneName_test,_ = dm.get_test(True, True)\n","\n","tf = pd.read_excel('transcription_factor2.xlsx')\n","\n","J_promoter = np.append(np.append(X_trainpromoter,X_validationpromoter,axis = 0),X_testpromoter,axis=0)\n","J_halflife = np.append(np.append(X_trainhalflife,X_validationhalflife,axis = 0),X_testhalflife,axis=0)\n","J_label = np.append(np.append(y_train,y_validation,axis = 0),y_test,axis=0)\n","J_genes = np.append(np.append(geneName_train,geneName_valid,axis = 0),geneName_test,axis=0)\n","J_genes = np.array(list(map(lambda x: x.decode('UTF-8'),J_genes)))\n","\n","mask_J = np.in1d(J_genes, tf[0])\n","J_halflife = J_halflife[mask_J]\n","J_genes = J_genes[mask_J]\n","J_promoter = J_promoter[mask_J]\n","J_label =J_label[mask_J]\n","\n","a = J_genes.argsort()\n","J_label = J_label.take(a,0)\n","J_promoter = J_promoter.take(a,0)\n","J_halflife = J_halflife.take(a,0)\n","J_genes.sort()\n","\n","tf = tf.sort_values(0)\n","tf = tf[tf[0].isin(J_genes)]\n","\n","import json\n","J_tf = np.array(list(map(lambda x: np.array(json.loads(x)), tf['TF'].values)))\n","\n","list_shuffle = list(range(len(J_label)))\n","print(list_shuffle)\n","######################################\n","np.random.seed(42)\n","######################################\n","np.random.shuffle(list_shuffle)\n","print(list_shuffle)\n","idx_test = list_shuffle[:1000]\n","idx_val = list_shuffle[1000:2000]\n","idx_train = list_shuffle[2000:]\n","\n","X_test_p = J_promoter[idx_test]\n","X_val_p = J_promoter[idx_val]\n","X_train_p = J_promoter[idx_train]\n","\n","X_test_h = J_halflife[idx_test]\n","X_val_h = J_halflife[idx_val]\n","X_train_h = J_halflife[idx_train]\n","\n","y_test = J_label[idx_test]\n","y_val = J_label[idx_val]\n","y_train = J_label[idx_train]\n","\n","X_test_tf = J_tf[idx_test]\n","X_val_tf = J_tf[idx_val]\n","X_train_tf = J_tf[idx_train]\n","\n","mask_tr = np.all(X_train_p < 4, axis = 1)\n","X_train_p = X_train_p[mask_tr]\n","X_train_h = X_train_h[mask_tr]\n","y_train = y_train[mask_tr]\n","X_train_tf = X_train_tf[mask_tr]\n","\n","mask_te = np.all(X_test_p < 4, axis = 1)\n","X_test_p = X_test_p[mask_te]\n","X_test_h = X_test_h[mask_te]\n","X_test_tf = X_test_tf[mask_te]\n","y_test = y_test[mask_te]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzapVaWZ8nsX"},"source":["Calculate new embedding"]},{"cell_type":"code","metadata":{"id":"Z5hY6xs-8mlx"},"source":["w2v = Word2Vec.load(\"embedded_data/word2vec.model\")\n","\n","X_train = embeddings(X_train_p)\n","X_validation = embeddings(X_val_p)\n","X_test = embeddings(X_test_p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGjvjt6y8vji"},"source":["Save the data"]},{"cell_type":"code","metadata":{"id":"fxItMsd38xbF"},"source":["  h5f = h5py.File('Dataset/embedded_data/etrain_tf.h5', 'w')\n","  h5f.create_dataset('promoter', data=X_train)\n","  h5f.create_dataset('halflife', data=X_train_h)\n","  h5f.create_dataset('tf',       data=X_train_tf)\n","  h5f.create_dataset('label',    data=y_train)\n","  h5f.close()\n","\n","  h5f = h5py.File('Dataset/embedded_data/etest_tf.h5', 'w')\n","  h5f.create_dataset('promoter', data=X_test)\n","  h5f.create_dataset('halflife', data=X_test_h)\n","  h5f.create_dataset('tf',       data=X_test_tf)\n","  h5f.create_dataset('label',    data=y_test)\n","  h5f.close()\n","\n","  h5f = h5py.File('Dataset/embedded_data/evalidation_tf.h5', 'w')\n","  h5f.create_dataset('promoter', data=X_validation)\n","  h5f.create_dataset('halflife', data=X_val_h)\n","  h5f.create_dataset('tf',       data=X_val_tf)\n","  h5f.create_dataset('label',    data=y_val)\n","  h5f.close()"],"execution_count":null,"outputs":[]}]}