{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufBVp5_EbvHJ","executionInfo":{"status":"ok","timestamp":1625479941448,"user_tz":-120,"elapsed":10,"user":{"displayName":"Vittorio Pipoli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjl5ImUIdl3WuIzbzHHEpYUFFgioDc_I1zwr7-FYw=s64","userId":"13197758426377553487"}},"outputId":"c789e4d4-130e-401a-fc0f-d670590fe511"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model, load_model\n","from scipy import stats\n","from matplotlib import pyplot\n","import numpy as np\n","import datetime, os\n","%pylab inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from IPython.display import display, Image\n","from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import regularizers\n","import math\n","import h5py\n","import pickle\n","import seaborn as sns\n","import pandas as pd\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","# Clear any logs from previous runs\n","!rm -rf ./logs/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n","The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MozLQRcetBDV"},"source":["# keras "]},{"cell_type":"code","metadata":{"id":"zTo7CP6IB2fk"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=8000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XTWRzMjj4KF"},"source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgnszfl-j4Ai"},"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4KGPxN8cIru"},"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCMvNdRhchFb"},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vnetQgIvfRc"},"source":["class TokenAndPositionEmbedding2(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding2, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions, tf.expand_dims(positions, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lydV7dFtpAms"},"source":["class TokenAndPositionEmbedding3(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n","        super(TokenAndPositionEmbedding3, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x) + positions\n","        x = self.dropout(x, training=training)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDmNdway3R6y"},"source":["class TokenAndPositionEmbedding4(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n","        super(TokenAndPositionEmbedding4, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_encoding = positional_encoding(maxlen,\n","                                            embed_dim)\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        self.d_model = embed_dim\n","\n","    def call(self, x, training):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.token_emb(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :maxlen, :]\n","        x = self.dropout(x, training=training)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19cPDo6rCwIA"},"source":["#only new_embedding\n","class PositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(PositionEmbedding, self).__init__()\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-2]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3QL17fPj581"},"source":["#only new_embedding\n","class PositionEncoding2(layers.Layer):\n","    def __init__(self, maxlen, embed_dim, rate):\n","        super(PositionEncoding2, self).__init__()\n","        self.pos_encoding = positional_encoding(maxlen,\n","                                            embed_dim)\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        self.d_model = embed_dim\n","\n","    def call(self, x, training):\n","        maxlen = tf.shape(x)[-2]\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :maxlen, :]\n","        x = self.dropout(x, training=training)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nos4dWnlb7MX"},"source":["class projTransformer:\n","    def __init__(   self,\n","                    checkpoint_dir=\"\",\n","                    model_type=\"best\",\n","                    n_epochs=300, \n","                    batch_size=32, \n","                    learning_rate=1e-4,\n","                    momentum=0.9,\n","                    maxlen=10500,\n","                    embed_dim=32,\n","                    num_heads=4,\n","                    ff_dim=64,\n","                    vocab_size=5,\n","                    dense=64,\n","                    lr_reduction_epoch=None,\n","                    dropout_rate=0.1,\n","                    t_rate = 0.1,\n","                    patience=20,\n","                    optimizer=\"SGD\",\n","                    warmup_steps = 8000,\n","                    shuffle = True,\n","                    loss = \"mse\",\n","                    logdir=None):\n","        \n","        self.checkpoint_dir     = checkpoint_dir\n","        self.model_type         = model_type\n","        self.n_epochs           = n_epochs\n","        self.batch_size         = batch_size\n","        self.learning_rate      = learning_rate\n","        self.momentum           = momentum\n","        self.maxlen             = maxlen\n","        self.embed_dim          = embed_dim\n","        self.num_heads          = num_heads\n","        self.ff_dim             = ff_dim\n","        self.vocab_size         = vocab_size\n","        self.dense              = dense\n","        self.dropout_rate       = dropout_rate\n","        self.lr_reduction_epoch = lr_reduction_epoch\n","        self.t_rate             = t_rate\n","        self.patience           = patience\n","        self.optimizer          = optimizer\n","        self.warmup_steps       = warmup_steps\n","        self.shuffle            = shuffle\n","        self.logdir             = logdir\n","        self.loss               = loss\n","        self.history            = \"\"\n","\n","        self._build_model()\n","\n","        #optimizer\n","        if self.optimizer == \"Adam\":\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n","        if self.optimizer == \"SGD\":\n","            optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n","        if self.optimizer == \"Adadelta\":\n","            optimizer = tf.keras.optimizers.Adadelta(learning_rate=self.learning_rate, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n","        if self.optimizer == \"Adamax\":\n","            optimizer = tf.keras.optimizers.Adamax(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n","        if self.optimizer == \"Original\":\n","            learning_rate = CustomSchedule(self.embed_dim, self.warmup_steps)\n","            optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","        # compile    \n","        self.model.compile(optimizer=optimizer, loss=self.loss)\n","\n","    def _build_model(self):\n","        if self.model_type == \"best\":\n","            embedding_layer = TokenAndPositionEmbedding(self.maxlen, self.vocab_size, self.embed_dim)\n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen))\n","            input2 = layers.Input(shape=(8))\n","\n","            #embedding\n","            embedded = embedding_layer(input1)\n"," \n","            #cnn layers\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n","            x = layers.AveragePooling1D(pool_size=30, strides=None, padding=\"valid\")(x)\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n","            x = layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\")(x)\n","\n","            x = layers.BatchNormalization()(x)\n","\n","            residual = layers.AveragePooling1D(pool_size=300, strides=None, padding=\"valid\")(embedded)\n","            residual = layers.BatchNormalization()(residual)\n","\n","            x = layers.Add()([x, residual])\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n"," \n","            #FC\n","            x = layers.Flatten()(x)\n","            x = layers.Concatenate()([x, input2])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #dense2\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        if self.model_type == \"globalP\":\n","            embedding_layer = TokenAndPositionEmbedding4(self.maxlen, self.vocab_size, self.embed_dim)\n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen))\n","            input2 = layers.Input(shape=(8))\n","\n","            #embedding\n","            embedded = embedding_layer(input1)\n"," \n","            #cnn layers\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n","            x = layers.AveragePooling1D(pool_size=30, strides=None, padding=\"valid\")(x)\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n","            x = layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\")(x)\n","\n","            x = layers.BatchNormalization()(x)\n","\n","            residual = layers.AveragePooling1D(pool_size=300, strides=None, padding=\"valid\")(embedded)\n","            residual = layers.BatchNormalization()(residual)\n","\n","            x = layers.Add()([x, residual])\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n"," \n","            #FC\n","            x = layers.GlobalAveragePooling1D()(x)\n","            # x = layers.Flatten()(x)\n","            x = layers.Concatenate()([x, input2])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #dense2\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","        \n","        \n","        if self.model_type == \"posSkip\":\n","            import tensorflow_probability as tfp\n","\n","            embedding_layer = TokenAndPositionEmbedding2(self.maxlen, self.vocab_size, self.embed_dim)\n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen))\n","            input2 = layers.Input(shape=(8))\n","\n","            #embedding\n","            embedded, positions = embedding_layer(input1)\n","\n","            print(tf.shape(embedded))\n","            print(tf.shape(positions))\n"," \n","            #cnn layers\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n","            x = layers.AveragePooling1D(pool_size=30, strides=None, padding=\"valid\", name=\"convAvg1\")(x)\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n","            x = layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\", name=\"convAvg2\")(x)\n","            x = layers.BatchNormalization()(x)\n","\n","            residual = layers.AveragePooling1D(pool_size=300, strides=None, padding=\"valid\")(positions)\n","            print(tf.shape(residual))\n","\n","            x = layers.Add()([x, residual])\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n"," \n","            #FC\n","            x = layers.Flatten()(x)\n","            x = layers.Concatenate()([x, input2])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #dense2\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        if self.model_type == \"DeepLncLoc\":\n","            embedding_layer = PositionEncoding2(self.maxlen, self.embed_dim, self.t_rate)\n","            # embedding_layer = PositionEmbedding(self.maxlen, self.vocab_size,self.embed_dim)\n","            \n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen, self.embed_dim))\n","            input2 = layers.Input(shape=(8))\n","\n","            x = input1\n","            #embedding\n","            x = layers.BatchNormalization()(x)\n","            x = embedding_layer(x)\n","\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n","\n","            #FC\n","            x = layers.GlobalAveragePooling1D()(x)\n","            # x = layers.Flatten()(x)\n","\n","            x = layers.Concatenate()([x, input2])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        if self.model_type == \"DeepLncLoc_TF\":\n","            embedding_layer = PositionEncoding2(self.maxlen, self.embed_dim, self.t_rate)\n","            # embedding_layer = PositionEmbedding(self.maxlen, self.vocab_size,self.embed_dim)\n","            \n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen, self.embed_dim))\n","            input2 = layers.Input(shape=(8))\n","            input3 = layers.Input(shape=(181))\n","\n","            x = input1\n","            #embedding\n","            x = layers.BatchNormalization()(x)\n","            x = embedding_layer(x)\n","\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n","\n","            #FC\n","            x = layers.GlobalAveragePooling1D()(x)\n","            # x = layers.Flatten()(x)\n","\n","            x = layers.Concatenate()([x, input2, input3])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2, input3],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        if self.model_type == \"DeepLncLoc_onlyPromo\":\n","            embedding_layer = PositionEncoding2(self.maxlen, self.embed_dim, self.t_rate)\n","            # embedding_layer = PositionEmbedding(self.maxlen, self.vocab_size,self.embed_dim)\n","            \n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen, self.embed_dim))\n","\n","            x = input1\n","            #embedding\n","            x = layers.BatchNormalization()(x)\n","            x = embedding_layer(x)\n","\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n","\n","            #FC\n","            x = layers.GlobalAveragePooling1D()(x)\n","            # x = layers.Flatten()(x)\n","\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        \n","\n","        if self.model_type == \"TF\":\n","            embedding_layer = TokenAndPositionEmbedding(self.maxlen, self.vocab_size, self.embed_dim)\n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen))\n","            input2 = layers.Input(shape=(8))\n","            input3 = layers.Input(shape=(181))\n","\n","            #embedding\n","            embedded = embedding_layer(input1)\n"," \n","            #cnn layers\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n","            x = layers.AveragePooling1D(pool_size=30, strides=None, padding=\"valid\")(x)\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n","            x = layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\")(x)\n","\n","            x = layers.BatchNormalization()(x)\n","            residual = layers.AveragePooling1D(pool_size=300, strides=None, padding=\"valid\")(embedded)\n","            residual = layers.BatchNormalization()(residual)\n","\n","            x = layers.Add()([x, residual])\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n"," \n","            #FC\n","            x = layers.Flatten()(x)\n","            x = layers.Concatenate()([x, input2, input3])\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #dense2\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1, input2, input3],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        if self.model_type == \"onlyPromo\":\n","            embedding_layer = TokenAndPositionEmbedding(self.maxlen, self.vocab_size, self.embed_dim)\n","            #inputs\n","            input1 = layers.Input(shape=(self.maxlen))\n","\n","            #embedding\n","            embedded = embedding_layer(input1)\n"," \n","            #cnn layers\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n","            x = layers.AveragePooling1D(pool_size=30, strides=None, padding=\"valid\")(x)\n","            x = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\", dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(x)\n","            x = layers.AveragePooling1D(pool_size=10, strides=None, padding=\"valid\")(x)\n","\n","            x = layers.BatchNormalization()(x)\n","            residual = layers.AveragePooling1D(pool_size=300, strides=None, padding=\"valid\")(embedded)\n","            residual = layers.BatchNormalization()(residual)\n","\n","            x = layers.Add()([x, residual])\n","            #transformers\n","            x = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate)(x)\n"," \n","            #FC\n","            x = layers.Flatten()(x)\n","\n","            #dense1\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #dense2\n","            x = layers.Dense(self.dense, activation=\"relu\")(x)\n","            x = layers.Dropout(self.dropout_rate)(x)\n","            #output\n","            output = layers.Dense(1, activation=\"linear\")(x)\n","\n","            print(\"model built\")\n","            self.model = tf.keras.Model(\n","                inputs=[input1],\n","                outputs=[output],\n","                )\n","            self.model.summary()\n","            img = tf.keras.utils.plot_model(self.model, \"multi_input_and_output_model.png\", show_shapes=True)\n","            display(img)\n","\n","        print(f\"\\nParameters:\\n{vars(self)}\\n\")\n","\n","    def train_model(self, x_train, y_train, x_val=None, y_val=None, TPU=False):\n","        #train test split\n","        if x_val is None:\n","            x_train, y_train, x_val, y_val = self._split_validation_data(x_train, y_train, 0.1)\n","\n","        history = tf.keras.callbacks.History()\n","        check_cb = ModelCheckpoint(os.path.join(f\"Saved_Models/checkpoint/{self.checkpoint_dir}\", f'bestmodel_transformer_{self.model_type}'), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","        earlystop_cb = EarlyStopping(monitor='val_loss', patience=self.patience, verbose=1, mode='min', restore_best_weights=TPU)\n","\n","        if TPU == True:\n","            if self.lr_reduction_epoch is not None and self.optimizer is not \"Original\":\n","                scheduler_callback = tf.keras.callbacks.LearningRateScheduler(self.lr_scheduler, verbose=1)\n","                callbacks = [history,\n","                                scheduler_callback,\n","                                earlystop_cb] \n","            else:\n","                callbacks = [history,\n","                                earlystop_cb]\n","        else:\n","            if self.lr_reduction_epoch is not None and self.optimizer is not \"Original\":\n","                scheduler_callback = tf.keras.callbacks.LearningRateScheduler(self.lr_scheduler, verbose=1)\n","                callbacks = [history,\n","                                check_cb,\n","                                scheduler_callback,\n","                                earlystop_cb]\n","            else:\n","                callbacks = [history,\n","                                check_cb,\n","                                earlystop_cb]\n","        if self.logdir is not None and TPU is not True:\n","            tensorboard_callback = tf.keras.callbacks.TensorBoard(self.logdir, \n","                                                        histogram_freq=1,\n","                                                        write_grads=True,\n","                                                        update_freq='epoch')\n","            callbacks.append(tensorboard_callback)\n","\n","        self.model.fit(x=x_train, \n","            y=y_train, \n","            shuffle=self.shuffle,\n","            epochs=self.n_epochs,\n","            batch_size=self.batch_size,\n","            validation_data=(x_val, y_val),\n","            callbacks=callbacks)\n","\n","        self.history = history\n","        plt.rcParams[\"figure.figsize\"] = (20,9)\n","        pyplot.plot(history.history['loss'])\n","        pyplot.plot(history.history['val_loss'])\n","        pyplot.hlines(0.4, 0, len(history.history['loss']) , alpha = 0.2)\n","        pyplot.hlines(0.42, 0, len(history.history['loss']) , alpha = 0.2 )\n","        pyplot.title('model train vs validation loss')\n","        pyplot.ylabel('loss')\n","        pyplot.xlabel('epoch')\n","        pyplot.legend(['train', 'validation'], loc='upper right')\n","        pyplot.show()\n","        \n","    def evaluate(self, x, y):\n","        predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","        print('Test R^2 = %.3f' % r_value**2)\n","\n","    def evaluate_best(self, x, y, TPU=False):\n","        if TPU is False:\n","            best_file = os.path.join(f\"Saved_Models/checkpoint/{self.checkpoint_dir}\", f'bestmodel_transformer_{self.model_type}')\n","            model = load_model(best_file)\n","            predictions = model.predict(x).flatten()\n","        else:\n","            predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","        print('Test R^2 = %.3f' % r_value**2)\n","        return r_value**2\n","\n","    def plot_kde(self, x, y, TPU=False):\n","        if TPU is False:\n","            best_file = os.path.join(f\"Saved_Models/checkpoint/{self.checkpoint_dir}\", f'bestmodel_transformer_{self.model_type}')\n","            model = load_model(best_file)\n","            predictions = model.predict(x).flatten()\n","        else:\n","            predictions = self.model.predict(x).flatten()\n","        df = pd.DataFrame({\"predictions\":predictions, \"true\":y})\n","        ax = sns.displot(data=df, kde=True)\n","        plt.xlabel(\"Labels\")\n","        plt.show()\n","\n","    def plot_train(self):\n","        history = self.history\n","        plt.rcParams[\"figure.figsize\"] = (20,9)\n","        pyplot.plot(history.history['loss'])\n","        pyplot.plot(history.history['val_loss'])\n","        pyplot.hlines(0.4, 0, len(history.history['loss']) , alpha = 0.2)\n","        pyplot.hlines(0.42, 0, len(history.history['loss']) , alpha = 0.2 )\n","        pyplot.title('model train vs validation loss')\n","        pyplot.ylabel('loss')\n","        pyplot.xlabel('epoch')\n","        pyplot.legend(['train', 'validation'], loc='upper right')\n","        pyplot.show()\n","\n","    def plot_r2(self, x, y, TPU=False):\n","        from matplotlib import cm\n","        if TPU == False:\n","            best_file = os.path.join(f\"Saved_Models/checkpoint/{self.checkpoint_dir}\", f'bestmodel_transformer_{self.model_type}')\n","            model = load_model(best_file)\n","            predictions = model.predict(x).flatten()\n","        else:\n","            predictions = self.model.predict(x).flatten()\n","        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n","\n","        viridis = cm.get_cmap('autumn', 12)\n","        diff = y - predictions\n","        diff = np.abs(diff)\n","\n","        ### plt size\n","        plt.rcParams[\"figure.figsize\"] = (10,9)\n","        ### plt fontsize\n","        plt.rcParams.update({'font.size': 16})\n","\n","        ### set title\n","        plt.title(\"Expression Scatterplot\")\n","        ### plot\n","        bis = np.arange(-1.5, 3, 2)\n","        plt.plot(bis, bis,  f\"b\", alpha=0.3)\n","        for p, yi, c in zip(predictions, y, diff):\n","            plt.plot(p, yi,  f\".\", markersize=10, color=viridis((1.0-c)/1.1))\n","        ### set ticks\n","        plt.xticks([i for i in range(-1, 4)])\n","        plt.yticks([i for i in range(-1, 4)])\n","        ### set labels\n","        plt.xlabel(\"Predicted expression level\")\n","        plt.ylabel(\"Median expression level\")\n","        ### create legend\n","        plt.legend(loc=\"upper right\", title=f\"r2 = %.3f\\n n = 1000\" % r_value**2)\n","        ### set ylim\n","        plt.ylim((-1.5,3))\n","        plt.xlim((-1.5,3))\n","        ### grid\n","        plt.grid(alpha=0.5)\n","        ### save\n","        # if self.save:\n","        #     plt.savefig(f\"{self.dir}{self.filename}.png\")\n","        ### show\n","        plt.show()\n","\n","    @staticmethod\n","    def _split_validation_data(x, y, validation_split):\n","        rand_indexes = np.random.permutation(x.shape[0])\n","        x = x[rand_indexes]\n","        y = y[rand_indexes]\n","        x_validation = x[:int(len(x) * validation_split)]\n","        y_validation = y[:int(len(x) * validation_split)]\n","        x_train = x[int(len(x) * validation_split):]\n","        y_train = y[int(len(x) * validation_split):]\n","        return x_train, y_train, x_validation, y_validation\n","\n","    def lr_scheduler(self, epoch, lr):\n","        if epoch == self.lr_reduction_epoch:\n","            return lr * 0.2\n","        else:\n","            return lr "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLmAtCmafOUk"},"source":["# model_type = \"best\"\n","# maxlen = 10500\n","# loss = \"mae\"\n","# logdir = \"posSkip/\"\n","\n","# net = projTransformer(checkpoint_dir=\"posSkip/\", model_type=model_type, n_epochs=300, batch_size=256, \n","#                     learning_rate=1e-3, patience=20, optimizer=\"SGD\", vocab_size=4,\n","#                     lr_reduction_epoch=60, maxlen=maxlen, embed_dim=32, num_heads=4, ff_dim=64, dense=64, \n","#                     dropout_rate=0.1, logdir=logdir, t_rate=0.1, momentum=0.9, loss=loss)"],"execution_count":null,"outputs":[]}]}